_index,DATE,ERR_export,ERR_warmup_eager,ITER,STEP,TIME_ITER,capability,cpu,date_start,device,device_name,discrepancies_abs,discrepancies_rel,dtype,dump_folder,dump_ort,dynamic,executable,exporter,filename,flag_fake_tensor,flag_no_grad,flag_training,has_cuda,input_size,machine,mema_gpu_0_before_loading,mema_gpu_1_after_loading,mema_gpu_2_after_warmup,mema_gpu_3_empty_cache,mema_gpu_4_after_repeat,mema_gpu_4_reset,mema_gpu_5_after_export,mema_gpu_6_after_gcollect,mema_gpu_7_after_session,mema_gpu_8_after_export_warmup,mema_gpu_9_after_export_repeat,memory_peak,model,model_name,nvtx,onnx_filesize,onnx_input_names,onnx_model,onnx_n_functions,onnx_n_initializer,onnx_n_inputs,onnx_n_nodes,onnx_n_outputs,onnx_n_sequence,onnx_n_sparse_initializer,onnx_output_names,op_onnx_Add,op_onnx_Cast,op_onnx_Concat,op_onnx_Conv,op_onnx_Div,op_onnx_Dropout,op_onnx_Erf,op_onnx_Expand,op_onnx_Gather,op_onnx_Identity,op_onnx_MatMul,op_onnx_Mul,op_onnx_Pow,op_onnx_ReduceMean,op_onnx_Reshape,op_onnx_Softmax,op_onnx_Sqrt,op_onnx_Sub,op_onnx_Transpose,op_onnx_Unsqueeze,op_onnx_initializer,op_onnx_sparse_initializer,op_torch_aten._unsafe_view.default,op_torch_aten.add.Tensor,op_torch_aten.cat.default,op_torch_aten.clone.default,op_torch_aten.conv2d.default,op_torch_aten.dropout.default,op_torch_aten.expand.default,op_torch_aten.gelu.default,op_torch_aten.layer_norm.default,op_torch_aten.linear.default,op_torch_aten.matmul.default,op_torch_aten.mul.Tensor,op_torch_aten.permute.default,op_torch_aten.scaled_dot_product_attention.default,op_torch_aten.select.int,op_torch_aten.slice.Tensor,op_torch_aten.softmax.int,op_torch_aten.transpose.int,op_torch_aten.unsqueeze.default,op_torch_aten.view.default,opt_patterns,output_data,output_size,params_dtype,params_size,part,processor,providers,quiet,repeat,speedup,speedup_increase,speedup_med,split_process,start,suite,target_opset,time_export,time_export_success,time_latency,time_latency_eager,time_latency_eager_t_max,time_latency_eager_t_med,time_latency_eager_t_min,time_latency_eager_t_qu,time_latency_eager_t_std,time_latency_t_max,time_latency_t_med,time_latency_t_min,time_latency_t_qu,time_latency_t_std,time_load,time_session,time_total,time_warmup,time_warmup_eager,verbose,version,version_onnx,version_onnxruntime,version_onnxscript,version_torch,version_transformers,warmup,ERROR,OUTPUT,CMD
beit_base_patch16_224-custom,2024-07-23,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.unbind'_ overload='int')>_ searched for ['aten::unbind_int'_ 'unbind_int'] and attributes ['__qualname__'_ '__name__']_ args=(permute_)_ kwargs={}_ --DEBUG--_ --SHAPE--_ dynamic_objects=_ dynamic_objects_rev=_ dynamic,,0,,14.302148916001897,7.0,40,2024-07-23,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,9633792,x86_64,0,466023424,466023424.0,466023424.0,466023424.0,215648768.0,,,,,,0,beit_base_patch16_224,beit_base_patch16_224,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,86530984,,x86_64,,1,30,,,,0,0,Timm,18,4.217174741002964,,,0.037665966567146825,0.04507277999800863,0.04486887600432965,0.009177020001516212,0.009177020001516212/0.009215450997726294/0.03419566020020286/0.04481039470192627/0.04484281499753706/0.04486887600432965/0.044906236996757797/0.044952188299794214/0.04497171799885109/0.04503880000265781/0.04507277999800863,0.01412950040621199,,,,,,2.30790395099757,,8.46013479699468,,0.05196026600024197,1,3.9.19,1.17.0,1.19.0+cu118,0.1.0.dev20240612,2.5.0.dev20240723+cu118,4.37.2,10,"onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info. --   warnings.warn(""WARNING: failed to get cudart_version from onnxruntime build info."") -- ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=beit_base_patch16_224 -- nvtx=0 -- opt_patterns= -- output_data= -- part= -- process=0 -- quiet=1 -- repeat=30 -- split_process=0 -- start=0 ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model beit_base_patch16_224 --exporter custom --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """
botnet26t_256-custom,2024-07-23,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,1,,12.789566380000906,7.0,40,2024-07-23,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,25165824,x86_64,0,352094720,1092524032.0,1092524032.0,1092524032.0,85890560.0,,,,,,0,botnet26t_256,botnet26t_256,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,12488672,,x86_64,,1,30,,,,0,0,Timm,18,4.3529318060027435,,,0.040063177733827615,0.045389384002191946,0.04531963249974069,0.006336152000585571,0.006336152000585571/0.008154225600446808/0.04527734200237319/0.04529806199934683/0.045312702996307054/0.04531963249974069/0.04532852300035302/0.0453322429995751/0.04533812320441939/0.04534965400016518/0.045389384002191946,0.012970086694252014,,,,,,0.6376000140007818,,6.94318718300201,,0.05230421619999106,1,3.9.19,1.17.0,1.19.0+cu118,0.1.0.dev20240612,2.5.0.dev20240723+cu118,4.37.2,10,"onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info. --   warnings.warn(""WARNING: failed to get cudart_version from onnxruntime build info."") -- ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=botnet26t_256 -- nvtx=0 -- opt_patterns= -- output_data= -- part= -- process=0 -- quiet=1 -- repeat=30 -- split_process=0 -- start=0 -- targe,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model botnet26t_256 --exporter custom --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" --memo"
cait_m36_384-custom,2024-07-23,,,2,last,88.1377262820024,7.0,40,2024-07-23,cuda,Tesla V100-SXM2-32GB,0.0078125,2.64453125,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,dump_bash_bench/cait_m36_384-custom-cuda-torch.float16/model.onnx,False,True,False,True,1769472,x86_64,0,1107655680,1107655680.0,1107655680.0,1107655680.0,587771392.0,594849792.0,587771392.0,587771392.0,587771392.0,587771392.0,0,cait_m36_384,cait_m36_384,0,483400.0,args_0,1.0,0.0,1313.0,1.0,2646.0,1.0,0.0,0.0,output_0,498.0,36.0,3.0,1.0,115.0,152.0,38.0,1.0,111.0,117.0,305.0,269.0,77.0,154.0,81.0,38.0,79.0,77.0,492.0,2.0,1313.0,0.0,36.0,77.0,3.0,36.0,1.0,152.0,1.0,38.0,77.0,229.0,72.0,112.0,186.0,2.0,111.0,3.0,36.0,75.0,2.0,45.0,,,4000.0,float16,271221352,,x86_64,CUDAExecutionProvider/CPUExecutionProvider,1,30,1.6471503164748293,0.6471503164748293,1.6983982569051312,0,0,Timm,18,15.836344832998293,15.838177675002953,0.08409070666748449,0.13851003409993912,0.14274121500056935,0.1426545124995755,0.03944534400216071,0.03944534400216071/0.1425815399023122/0.14261005079752068/0.14262003780022497/0.14263169140176615/0.1426545124995755/0.14266786319785751/0.14267505270181574/0.1426893330004532/0.14270748309718329/0.14274121500056935,0.01878077741646095,0.08476742600032594,0.08399355800065678,0.08389410500240047,0.08389410500240047/0.08390433500098879/0.08392302599822869/0.08393659670473426/0.08396386700187577/0.08399355800065678/0.08404784900340019/0.08412200099774055/0.08423325300391298/0.08440982809406705/0.08476742600032594,0.0002220708892050952,6.903367953003908,2.2404059160035104,81.53429933499865,4.784068849399773,0.14744528649971472,1,3.9.19,1.17.0,1.19.0+cu118,0.1.0.dev20240612,2.5.0.dev20240723+cu118,4.37.2,10,"onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info. --   warnings.warn(""WARNING: failed to get cudart_version from onnxruntime build info."") -- [0;93m2024-07-23 12:13",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=cait_m36_384 -- nvtx=0 -- opt_patterns= -- output_data= -- part= -- process=0 -- quiet=1 -- repeat=30 -- split_process=0 -- start=0 -- target,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model cait_m36_384 --exporter custom --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" --memor"
coat_lite_mini-custom,2024-07-23,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.unbind'_ overload='int')>_ searched for ['aten::unbind_int'_ 'unbind_int'] and attributes ['__qualname__'_ '__name__']_ args=(permute_)_ kwargs={}_ --DEBUG--_ --SHAPE--_ dynamic_objects=_ dynamic_objects_rev=_ dynamic,,3,,13.770329398001195,7.0,40,2024-07-23,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,19267584,x86_64,0,276847616,1049474560.0,1049474560.0,1049474560.0,69842432.0,,,,,,0,coat_lite_mini,coat_lite_mini,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,11011560,,x86_64,,1,30,,,,0,0,Timm,18,4.984182541993505,,,0.04200648779951734,0.046809018997009844,0.04664256549949641,0.012304193995078094,0.012304193995078094/0.015248094499111176/0.04654663299879758/0.04658298400172498/0.04662705499649746/0.04664256549949641/0.0466595160018187/0.046686126000713556/0.046705676197598224/0.046724547899066236/0.046809018997009844,0.011371872384003398,,,,,,0.7431794709991664,,7.717728607996833,,0.05191712149971863,1,3.9.19,1.17.0,1.19.0+cu118,0.1.0.dev20240612,2.5.0.dev20240723+cu118,4.37.2,10,"onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info. --   warnings.warn(""WARNING: failed to get cudart_version from onnxruntime build info."") -- ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=coat_lite_mini -- nvtx=0 -- opt_patterns= -- output_data= -- part= -- process=0 -- quiet=1 -- repeat=30 -- split_process=0 -- start=0 -- targ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model coat_lite_mini --exporter custom --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" --mem"
convit_base-custom,2024-07-23,,expected mat1 and mat2 to have the same dtype_ but got: float != c10::Half,4,,8.101933344994904,7.0,40,2024-07-23,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,9633792,x86_64,0,462303232,,,,,,,,,,0,convit_base,convit_base,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,float16,86540040,,x86_64,,1,30,,,,0,0,Timm,18,,,,,,,,,,,,,,,2.124106720999407,,2.3296633110003313,,0.014044694099720801,1,3.9.19,1.17.0,1.19.0+cu118,0.1.0.dev20240612,2.5.0.dev20240723+cu118,4.37.2,10,"onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info. --   warnings.warn(""WARNING: failed to get cudart_version from onnxruntime build info."") -- ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=convit_base -- nvtx=0 -- opt_patterns= -- output_data= -- part= -- process=0 -- quiet=1 -- repeat=30 -- split_process=0 -- start=0 -- target_,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convit_base --exporter custom --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" --memory"
convmixer_768_32-custom,2024-07-23,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,5,,16.26773406299617,7.0,40,2024-07-23,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,4816896,x86_64,0,142677504,266273792.0,266273792.0,266273792.0,64941056.0,,,,,,0,convmixer_768_32,convmixer_768_32,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000.0,float16,21110248,,x86_64,,1,30,,,,0,0,Timm,18,5.269235735999246,,,0.10072858539958057,0.1105611260063597,0.11032525349946809,0.00849949899566127,0.00849949899566127/0.10147878929637957/0.1099070232041413/0.11016475339711178/0.11024978499917779/0.11032525349946809/0.11036484780051978/0.11037882859818637/0.11042703540151706/0.11047535079997033/0.1105611260063597,0.028741868272942495,,,,,,0.7817148290050682,,10.401231079995341,,0.09716743840035633,1,3.9.19,1.17.0,1.19.0+cu118,0.1.0.dev20240612,2.5.0.dev20240723+cu118,4.37.2,10,"onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info. --   warnings.warn(""WARNING: failed to get cudart_version from onnxruntime build info."") -- ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=convmixer_768_32 -- nvtx=0 -- opt_patterns= -- output_data= -- part= -- process=0 -- quiet=1 -- repeat=30 -- split_process=0 -- start=0 -- ta,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convmixer_768_32 --exporter custom --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" --m"
convnext_base-custom,2024-07-23,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.adaptive_avg_pool2d'_ overload='default')>_ searched for ['aten::adaptive_avg_pool2d'_ 'adaptive_avg_pool2d_default'] and attributes ['__qualname__'_ '__name__']_ args=(add_35_ [1_ 1])_ kwargs={}_ --DEBUG--_ --SHAPE--,,6,,18.95923833899724,7.0,40,2024-07-23,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,15925248,x86_64,0,545994240,1154036224.0,1154036224.0,1154036224.0,217657856.0,,,,,,0,convnext_base,convnext_base,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,88591464,,x86_64,,1,30,,,,0,0,Timm,18,7.328993545997946,,,0.0756924190003095,0.08509297700220486,0.08468951749819098,0.014112010998360347,0.014112010998360347/0.023078048403840513/0.08441762160364305/0.08455412439798238/0.08466696699906606/0.08468951749819098/0.08473370839928976/0.08475930830172729/0.0848672711988911/0.08490976210305234/0.08509297700220486,0.023020043924447883,,,,,,2.4002094800016494,,13.067696468999202,,0.07226448619985604,1,3.9.19,1.17.0,1.19.0+cu118,0.1.0.dev20240612,2.5.0.dev20240723+cu118,4.37.2,10,"onnxruntime/build/linux_cuda/Release/onnxruntime/capi/onnxruntime_validation.py:114: UserWarning: WARNING: failed to get cudart_version from onnxruntime build info. --   warnings.warn(""WARNING: failed to get cudart_version from onnxruntime build info."") -- ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=convnext_base -- nvtx=0 -- opt_patterns= -- output_data= -- part= -- process=0 -- quiet=1 -- repeat=30 -- split_process=0 -- start=0 -- targe,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convnext_base --exporter custom --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" --memo"
