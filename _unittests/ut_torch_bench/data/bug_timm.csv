_index, 6 : RUNTIME_EXCEPTION : Exception during initialization: github/onnxruntime/onnxruntime/core/providers/cuda/nn/batch_norm.h:44 onnxruntime::cuda::BatchNorm<T,DATE,ERR_export,ERR_ort,ERR_warmup_eager,ITER,TIME_ITER,capability,cpu,date_start,device,device_name,discrepancies_abs,discrepancies_rel,dtype,dump_folder,dump_ort,dynamic,executable,exporter,filename,flag_fake_tensor,flag_no_grad,flag_training,has_cuda,input_size,machine,mema_gpu_0_before_loading,mema_gpu_1_after_loading,mema_gpu_2_after_warmup,mema_gpu_3_empty_cache,mema_gpu_4_after_repeat,mema_gpu_5_after_export,mema_gpu_6_after_gcollect,mema_gpu_7_after_session,mema_gpu_8_after_export_warmup,mema_gpu_9_after_export_repeat,memory_peak,model,model_name,nvtx,onnx_filesize,onnx_input_names,onnx_model,onnx_n_functions,onnx_n_initializer,onnx_n_inputs,onnx_n_nodes,onnx_n_outputs,onnx_n_sequence,onnx_n_sparse_initializer,onnx_optimized,onnx_output_names,op_onnx_Add,op_onnx_AveragePool,op_onnx_BatchNormalization,op_onnx_Cast,op_onnx_Concat,op_onnx_Constant,op_onnx_ConstantOfShape,op_onnx_Conv,op_onnx_Div,op_onnx_Dropout,op_onnx_Equal,op_onnx_Erf,op_onnx_Expand,op_onnx_Flatten,op_onnx_Gather,op_onnx_GatherND,op_onnx_Gemm,op_onnx_GlobalAveragePool,op_onnx_HardSigmoid,op_onnx_HardSwish,op_onnx_Identity,op_onnx_LayerNormalization,op_onnx_LeakyRelu,op_onnx_MatMul,op_onnx_MaxPool,op_onnx_Mul,op_onnx_Pad,op_onnx_Pow,op_onnx_ReduceMean,op_onnx_Relu,op_onnx_Reshape,op_onnx_Resize,op_onnx_Shape,op_onnx_Sigmoid,op_onnx_Slice,op_onnx_Softmax,op_onnx_Split,op_onnx_Sqrt,op_onnx_Squeeze,op_onnx_Sub,op_onnx_Transpose,op_onnx_Unsqueeze,op_onnx_Where,op_onnx_initializer,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_17,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_18,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_19,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_20,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_21,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_22,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_23,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_24,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_25,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_26,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_27,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_28,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_29,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_30,op_onnx_pkg.onnxscript.torch_lib_aten_add|folded_31,op_onnx_pkg.timm.1.0.7_timm_layers_adaptive_avgmax_pool_SelectAdaptivePool2d_model_pooling_1,op_onnx_pkg.timm.1.0.7_timm_layers_classifier_ClassifierHead_model_head_1,op_onnx_pkg.timm.1.0.7_timm_layers_classifier_NormMlpClassifierHead_model_head_1,op_onnx_pkg.timm.1.0.7_timm_layers_linear_Linear_model_classifier_1,op_onnx_pkg.timm.1.0.7_timm_layers_norm_LayerNorm2d_model_stem_1_1,op_onnx_pkg.timm.1.0.7_timm_layers_patch_embed_PatchEmbed_model_patch_embed1_1,op_onnx_pkg.timm.1.0.7_timm_layers_patch_embed_PatchEmbed_model_patch_embed2_1,op_onnx_pkg.timm.1.0.7_timm_layers_patch_embed_PatchEmbed_model_patch_embed3_1,op_onnx_pkg.timm.1.0.7_timm_layers_patch_embed_PatchEmbed_model_patch_embed4_1,op_onnx_pkg.timm.1.0.7_timm_layers_patch_embed_PatchEmbed_model_patch_embed_1,op_onnx_pkg.timm.1.0.7_timm_layers_std_conv_ScaledStdConv2dSame_model_final_conv_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_0_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_10_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_11_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_1_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_2_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_3_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_4_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_5_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_6_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_7_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_8_1,op_onnx_pkg.timm.1.0.7_timm_models_beit_Block_model_blocks_9_1,op_onnx_pkg.timm.1.0.7_timm_models_byobnet_Stem_model_stem_1,op_onnx_pkg.timm.1.0.7_timm_models_cait_LayerScaleBlockClassAttn_model_blocks_token_only_0_1,op_onnx_pkg.timm.1.0.7_timm_models_cait_LayerScaleBlockClassAttn_model_blocks_token_only_1_1,op_onnx_pkg.timm.1.0.7_timm_models_coat_SerialBlock_model_serial_blocks1_0_1,op_onnx_pkg.timm.1.0.7_timm_models_coat_SerialBlock_model_serial_blocks1_1_1,op_onnx_pkg.timm.1.0.7_timm_models_coat_SerialBlock_model_serial_blocks2_0_1,op_onnx_pkg.timm.1.0.7_timm_models_coat_SerialBlock_model_serial_blocks2_1_1,op_onnx_pkg.timm.1.0.7_timm_models_coat_SerialBlock_model_serial_blocks3_0_1,op_onnx_pkg.timm.1.0.7_timm_models_coat_SerialBlock_model_serial_blocks3_1_1,op_onnx_pkg.timm.1.0.7_timm_models_coat_SerialBlock_model_serial_blocks4_0_1,op_onnx_pkg.timm.1.0.7_timm_models_coat_SerialBlock_model_serial_blocks4_1_1,op_onnx_pkg.timm.1.0.7_timm_models_crossvit_MultiScaleBlock_model_blocks_0_1,op_onnx_pkg.timm.1.0.7_timm_models_crossvit_MultiScaleBlock_model_blocks_1_1,op_onnx_pkg.timm.1.0.7_timm_models_crossvit_MultiScaleBlock_model_blocks_2_1,op_onnx_pkg.timm.1.0.7_timm_models_crossvit_PatchEmbed_model_patch_embed_0_1,op_onnx_pkg.timm.1.0.7_timm_models_crossvit_PatchEmbed_model_patch_embed_1_1,op_onnx_pkg.timm.1.0.7_timm_models_dla_DlaTree_model_level2_1,op_onnx_pkg.timm.1.0.7_timm_models_dla_DlaTree_model_level3_1,op_onnx_pkg.timm.1.0.7_timm_models_dla_DlaTree_model_level4_1,op_onnx_pkg.timm.1.0.7_timm_models_dla_DlaTree_model_level5_1,op_onnx_pkg.timm.1.0.7_timm_models_nfnet_GammaAct_model_final_act_1,op_onnx_pkg.torch.2.5.0.dev20240712+cu118_torch_nn_modules_container_Sequential_model_base_layer_1,op_onnx_pkg.torch.2.5.0.dev20240712+cu118_torch_nn_modules_container_Sequential_model_blocks_1,op_onnx_pkg.torch.2.5.0.dev20240712+cu118_torch_nn_modules_container_Sequential_model_features_1,op_onnx_pkg.torch.2.5.0.dev20240712+cu118_torch_nn_modules_container_Sequential_model_level0_1,op_onnx_pkg.torch.2.5.0.dev20240712+cu118_torch_nn_modules_container_Sequential_model_level1_1,op_onnx_pkg.torch.2.5.0.dev20240712+cu118_torch_nn_modules_container_Sequential_model_stages_1,op_onnx_pkg.torch.2.5.0.dev20240712+cu118_torch_nn_modules_container_Sequential_model_stem_1,op_onnx_sparse_initializer,op_torch_aten._unsafe_view.default,op_torch_aten.add.Tensor,op_torch_aten.cat.default,op_torch_aten.clone.default,op_torch_aten.conv2d.default,op_torch_aten.dropout.default,op_torch_aten.expand.default,op_torch_aten.gelu.default,op_torch_aten.layer_norm.default,op_torch_aten.linear.default,op_torch_aten.matmul.default,op_torch_aten.mul.Tensor,op_torch_aten.permute.default,op_torch_aten.scaled_dot_product_attention.default,op_torch_aten.select.int,op_torch_aten.slice.Tensor,op_torch_aten.softmax.int,op_torch_aten.transpose.int,op_torch_aten.unsqueeze.default,op_torch_aten.view.default,opt_patterns,output_data,output_size,params_dtype,params_size,process,processor,providers,quiet,repeat,speedup,speedup_increase,start,suite,target_opset,time_export,time_latency,time_latency_eager,time_load,time_session,time_total,time_warmup,time_warmup_eager,verbose,version,version_onnxruntime,version_torch,version_transformers,warmup,ERROR,OUTPUT,CMD
beit_base_patch16_224-script,,2024-07-12,,,,0,15.1667765279999,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.01416015625,3.705078125,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/beit_base_patch16_224-script-cuda-torch.float16/model.onnx,False,True,False,True,9633792.0,x86_64,0.0,206305792.0,215742976.0,215742976.0,215742976.0,215742976.0,28705280.0,28705280.0,28705280.0,28705280.0,0,beit_base_patch16_224,beit_base_patch16_224,0,173549371.0,input.1,1.0,0.0,199.0,1.0,1079.0,1.0,0.0,0.0,0.0,1302,96.0,,,36.0,39.0,309.0,1.0,1.0,24.0,,1.0,12.0,1.0,,12.0,,1.0,,,,,25.0,,72.0,,73.0,,,1.0,,38.0,,14.0,,14.0,12.0,12.0,36.0,36.0,,97.0,115.0,1.0,199.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,86530984.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.6877911957327287,-0.31220880426727127,0,Timm,18,2.439602118975017,0.05309802576666698,0.03652035463310312,1.4735231240047142,0.8880455570179038,9.10667543701129,0.06674532659817486,0.0397135465987958,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/__init__.py:1997: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values_ so this value will be treated as a constant in the future. This means that the trace ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=beit_base_patch16_224 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbos,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model beit_base_patch16_224 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --o"
beit_base_patch16_224-dynamo2,,2024-07-12,,,,1,30.2540846769989,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,3.640625,2536.0,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/beit_base_patch16_224-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,9633792.0,x86_64,0.0,206416384.0,215853568.0,215853568.0,215853568.0,215853568.0,28705280.0,28705280.0,28705280.0,28705280.0,0,beit_base_patch16_224,beit_base_patch16_224,0,433099.0,l_args_0_,1.0,97.0,222.0,1.0,30.0,1.0,0.0,0.0,0.0,model_head_1,,,,,1.0,10.0,,,,,,,,,,,1.0,,,,,1.0,,,,,,,1.0,,,,,,2.0,,,,,,1.0,,,222.0,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,86530984.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.6982897049462075,-0.30171029505379254,0,Timm,18,17.765396212984342,0.05224903146639311,0.03648496076639276,1.5168611869739834,0.7158237209951039,24.169671431998722,0.06610494309861678,0.03856116500101052,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=beit_base_patch16_224 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbo,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model beit_base_patch16_224 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --"
beit_base_patch16_224-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.unbind'_ overload='int')>_ searched for ['aten::unbind_int'_ 'unbind_int'] and attributes ['__qualname__'_ '__name__']_ args=(permute_)_ kwargs={}_ --DEBUG--_ --SHAPE--_ dynamic_objects={}_ dynamic_objects_rev={}_ dyn,,,2,13.636522764019901,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,9633792.0,x86_64,0.0,206305792.0,215742976.0,215742976.0,215742976.0,,,,,,0,beit_base_patch16_224,beit_base_patch16_224,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,86530984.0,0,x86_64,,1,30,,,0,Timm,18,4.323518999997759,,0.036774369033325156,1.49581845200737,,,,0.040040249199955726,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=beit_base_patch16_224 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbos,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model beit_base_patch16_224 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --o"
botnet26t_256-script,,2024-07-12,,,,3,13.875719503994333,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0185546875,8.578125,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/botnet26t_256-script-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,77370880.0,85890560.0,85890560.0,85890560.0,85890560.0,58852352.0,58852352.0,58852352.0,58852352.0,0,botnet26t_256,botnet26t_256,0,25306225.0,input.1,1.0,0.0,163.0,1.0,1106.0,1.0,0.0,0.0,0.0,1307,14.0,1.0,31.0,24.0,66.0,419.0,18.0,31.0,,,6.0,,6.0,7.0,12.0,,1.0,1.0,,,,,,12.0,1.0,21.0,12.0,,,27.0,66.0,,18.0,,24.0,3.0,3.0,,,12.0,36.0,228.0,6.0,163.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,12488672.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.8675200254189727,-0.13247997458102734,0,Timm,18,0.8341385029780213,0.04615035229944624,0.04003635479991014,0.48470104701118544,0.17251789299189113,7.510890200996073,0.25643362700066064,0.05033536489936523,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/__init__.py:1997: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values_ so this value will be treated as a constant in the future. This means that the trace ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=botnet26t_256 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- w,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model botnet26t_256 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_da"
botnet26t_256-dynamo2,,2024-07-12,,,,4,20.732364683994092,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0185546875,8.578125,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/botnet26t_256-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,77370880.0,85890560.0,85890560.0,85890560.0,85890560.0,58852352.0,58852352.0,58852352.0,58852352.0,0,botnet26t_256,botnet26t_256,0,389299.0,l_args_0_,1.0,42.0,163.0,1.0,3.0,1.0,0.0,0.0,0.0,model_head_1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,163.0,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,12488672.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.8546820528538623,-0.14531794714613766,0,Timm,18,7.70842287800042,0.04684181456686929,0.040034858233411796,0.5025443369813729,0.17742997099412605,14.440947041992331,0.2552444700995693,0.05010426130029373,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=botnet26t_256 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model botnet26t_256 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_d"
botnet26t_256-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,5,12.812310161010828,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,25165824.0,x86_64,0.0,77370880.0,85890560.0,85890560.0,85890560.0,,,,,,0,botnet26t_256,botnet26t_256,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,12488672.0,0,x86_64,,1,30,,,0,Timm,18,4.351559820002876,,0.04005515750031918,0.4863875930022914,,,,0.05113146980002057,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=botnet26t_256 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- w,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model botnet26t_256 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_da"
cait_m36_384-script,,2024-07-12,,,,6,30.779669504001504,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.015625,2.828125,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/cait_m36_384-script-cuda-torch.float16/model.onnx,False,True,False,True,1769472.0,x86_64,0.0,578895360.0,587415040.0,587415040.0,587415040.0,587415040.0,12976640.0,12976640.0,12976640.0,12976640.0,0,cait_m36_384,cait_m36_384,0,542932903.0,input.1,1.0,0.0,692.0,1.0,2788.0,1.0,0.0,0.0,0.0,3480,341.0,,,6.0,85.0,644.0,1.0,1.0,40.0,,1.0,38.0,1.0,,111.0,,3.0,,,,,77.0,,302.0,,193.0,,,,,82.0,,4.0,,3.0,38.0,,6.0,,,487.0,323.0,1.0,692.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,4000.0,float16,271221352.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.648692798305999,0.6486927983059989,0,Timm,18,7.828672742994968,0.08400905206605482,0.13850511913381827,3.9954738770029508,2.622626094002044,24.334646670002257,0.0958859690988902,0.1461218635988189,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/__init__.py:1997: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values_ so this value will be treated as a constant in the future. This means that the trace ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=cait_m36_384 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- wa,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model cait_m36_384 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_dat"
cait_m36_384-dynamo2,,2024-07-12,,,,7,70.64384269400034,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.017578125,3.447265625,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/cait_m36_384-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,1769472.0,x86_64,0.0,579812864.0,588332544.0,588332544.0,588332544.0,588332544.0,12976640.0,12976640.0,12976640.0,12976640.0,0,cait_m36_384,cait_m36_384,0,2268687.0,l_args_0_,1.0,382.0,690.0,1.0,18.0,1.0,0.0,0.0,0.0,model_head_1,1.0,,,,1.0,7.0,,,,,,,,,1.0,,1.0,,,,,1.0,,,,,,,,,,,,,1.0,,,,,,1.0,,,690.0,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,1.0,1.0,,,,,,,,,,,,,,,,,,,,1.0,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,4000.0,float16,271221352.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.65899104855888,0.6589910485588799,0,Timm,18,47.8728756759956,0.0835316237663695,0.13857821609999518,4.014730087015778,2.5422467210155446,64.09355576199596,0.09855017820082139,0.14764051150123123,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version_ please register them with register_custom_op. --   warnings.warn( -- ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=cait_m36_384 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- w,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model cait_m36_384 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_da"
cait_m36_384-custom,,2024-07-12,,,,8,38.178066745982505,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0078125,2.64453125,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,dump_test_models/cait_m36_384-custom-cuda-torch.float16/model.onnx,False,True,False,True,1769472.0,x86_64,0.0,577060352.0,585580032.0,585580032.0,585580032.0,585580032.0,585580032.0,585580032.0,585580032.0,585580032.0,0,cait_m36_384,cait_m36_384,0,479981.0,args_0,1.0,0.0,1313.0,1.0,2646.0,1.0,0.0,0.0,,output_0,498.0,,,36.0,3.0,,,1.0,115.0,152.0,,38.0,1.0,,111.0,,,,,,117.0,,,305.0,,269.0,,77.0,154.0,,81.0,,,,,38.0,,79.0,,77.0,492.0,2.0,,1313.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,36.0,77.0,3.0,36.0,1.0,152.0,1.0,38.0,77.0,229.0,72.0,112.0,186.0,2.0,111.0,3.0,36.0,75.0,2.0,45.0,,,4000.0,float16,271221352.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.6498172980116859,0.6498172980116859,0,Timm,18,15.64544746701722,0.08399725799972657,0.13858012923349936,4.053879629005678,2.2964859060011804,31.55601890399703,0.10051736099994742,0.1461579016991891,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,[0;93m2024-07-12 16:36:25.361026886 [W:onnxruntime:_ constant_folding.cc:269 ApplyImpl] Could not find a CPU kernel and hence can't constant fold Sqrt node 'aten_scaled_dot_product_attention7'[m -- [0;93m2024-07-12 16:36:26.971081113 [W:onnxruntime:_ constant_folding.cc:269 ApplyImpl] Could not find,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=cait_m36_384 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- wa,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model cait_m36_384 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_dat"
coat_lite_mini-script,,2024-07-12,,,,9,15.321801489015343,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0078125,2.572265625,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/coat_lite_mini-script-cuda-torch.float16/model.onnx,False,True,False,True,19267584.0,x86_64,0.0,61322752.0,69842432.0,69842432.0,69842432.0,69842432.0,47055872.0,47055872.0,47055872.0,47055872.0,0,coat_lite_mini,coat_lite_mini,0,22336070.0,x.1,1.0,0.0,152.0,1.0,1640.0,1.0,0.0,0.0,0.0,1825,72.0,,,16.0,87.0,621.0,12.0,36.0,8.0,,4.0,8.0,4.0,,9.0,,1.0,,,,,21.0,,48.0,,44.0,8.0,,,,75.0,,24.0,,55.0,8.0,16.0,,24.0,8.0,103.0,324.0,4.0,152.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,11011560.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.7723283838978314,-0.22767161610216857,0,Timm,18,1.1606716930109542,0.054271282666013575,0.04191525203350466,0.4859952830011025,0.2586158539925236,8.961941698973533,0.33076257079956123,0.04987456939998083,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/timm/models/_hub.py:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value)_ which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary c,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=coat_lite_mini -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model coat_lite_mini --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_d"
coat_lite_mini-dynamo2,,2024-07-12,,,,10,24.819680512009654,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0078125,2.560546875,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/coat_lite_mini-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,19267584.0,x86_64,0.0,61322752.0,69842432.0,69842432.0,69842432.0,69842432.0,47055872.0,47055872.0,47055872.0,47055872.0,0,coat_lite_mini,coat_lite_mini,0,9303419.0,l_args_0_,1.0,92.0,147.0,1.0,84.0,1.0,0.0,0.0,0.0,model_head_1,,,,,4.0,49.0,,,,,,,,,1.0,,1.0,,,,,1.0,,,,,,,,,3.0,,,,10.0,,,,,,3.0,,,147.0,,,,,,,,,,,,,,,,,,,,,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,11011560.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.7735333261809713,-0.22646667381902874,0,Timm,18,10.66433199599851,0.05424390423383253,0.04195946766703855,0.48788483199314214,0.28620987202157266,18.53076768098981,0.3307043834007345,0.04986535119824111,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/timm/models/_hub.py:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value)_ which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary c,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=coat_lite_mini -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 --,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model coat_lite_mini --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_"
coat_lite_mini-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.unbind'_ overload='int')>_ searched for ['aten::unbind_int'_ 'unbind_int'] and attributes ['__qualname__'_ '__name__']_ args=(permute_)_ kwargs={}_ --DEBUG--_ --SHAPE--_ dynamic_objects={}_ dynamic_objects_rev={}_ dyn,,,11,13.634268655005144,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,19267584.0,x86_64,0.0,61322752.0,69842432.0,69842432.0,69842432.0,,,,,,0,coat_lite_mini,coat_lite_mini,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,11011560.0,0,x86_64,,1,30,,,0,Timm,18,4.936547033983516,,0.04193945029983297,0.5214000870182645,,,,0.050387209598557095,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/timm/models/_hub.py:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value)_ which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary c,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=coat_lite_mini -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model coat_lite_mini --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_d"
convit_base-script,,2024-07-12,,,expected mat1 and mat2 to have the same dtype_ but got: float != c10::Half,12,7.739459197997348,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,script,,False,True,False,True,9633792.0,x86_64,0.0,204625408.0,,,,,,,,,0,convit_base,convit_base,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,float16,86540040.0,0,x86_64,,1,30,,,0,Timm,18,,,,1.496195236017229,,,,0.014109784201718868,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=convit_base -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- war,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convit_base --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data"
convit_base-dynamo2,,2024-07-12,,,expected mat1 and mat2 to have the same dtype_ but got: float != c10::Half,13,7.679609463986708,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,,False,True,False,True,9633792.0,x86_64,0.0,204625408.0,,,,,,,,,0,convit_base,convit_base,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,float16,86540040.0,0,x86_64,,1,30,,,0,Timm,18,,,,1.4373869789997116,,,,0.013923856502515264,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=convit_base -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- wa,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convit_base --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_dat"
convit_base-custom,,2024-07-12,,,expected mat1 and mat2 to have the same dtype_ but got: float != c10::Half,14,7.6462634860072285,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,9633792.0,x86_64,0.0,204625408.0,,,,,,,,,0,convit_base,convit_base,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,float16,86540040.0,0,x86_64,,1,30,,,0,Timm,18,,,,1.4364991890033707,,,,0.013751328698708676,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=convit_base -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- war,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convit_base --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data"
convmixer_768_32-script,,2024-07-12,,,,15,32.404394230019534,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0087890625,0.00917816162109375,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/convmixer_768_32-script-cuda-torch.float16/model.onnx,False,True,False,True,4816896.0,x86_64,0.0,56421376.0,64941056.0,64941056.0,64941056.0,64941056.0,18153984.0,18153984.0,18153984.0,18153984.0,0,convmixer_768_32,convmixer_768_32,0,42500071.0,input.1,1.0,0.0,392.0,1.0,230.0,1.0,0.0,0.0,0.0,687,32.0,,65.0,,,,,65.0,,,,,,1.0,,,1.0,1.0,,,,,,,,,,,,65.0,,,,,,,,,,,,,,392.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,32000.0,float16,21110248.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.32031905452458,0.32031905452458,0,Timm,18,2.1473565259948373,0.07589306356676388,0.10020305793344354,0.5255183359840885,0.11486239999067038,26.027624895010376,1.648067916400032,0.09659049560141283,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=convmixer_768_32 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convmixer_768_32 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output"
convmixer_768_32-dynamo2,,2024-07-12,,,,16,40.8572617440077,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0087890625,0.00917816162109375,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/convmixer_768_32-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,4816896.0,x86_64,0.0,56421376.0,64941056.0,64941056.0,64941056.0,64941056.0,18153984.0,18153984.0,18153984.0,18153984.0,0,convmixer_768_32,convmixer_768_32,0,405578.0,l_args_0_,1.0,67.0,392.0,1.0,5.0,1.0,0.0,0.0,0.0,model_head_1,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,392.0,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,1.0,0.0,,,,,,,,,,,,,,,,,,,,,,,32000.0,float16,21110248.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.3215949994756178,0.32159499947561776,0,Timm,18,9.572691312001552,0.07585556856647599,0.10025034009983452,0.5138185279793106,0.15708418400026858,34.36416086999816,1.7315734367992264,0.09883521019946784,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=convmixer_768_32 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convmixer_768_32 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --outpu"
convmixer_768_32-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,17,16.13799507598742,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,4816896.0,x86_64,0.0,56421376.0,64941056.0,64941056.0,64941056.0,,,,,,0,convmixer_768_32,convmixer_768_32,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,32000.0,float16,21110248.0,0,x86_64,,1,30,,,0,Timm,18,5.102388400002383,,0.10031985276727937,0.474300245026825,,,,0.09659199759771582,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=convmixer_768_32 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convmixer_768_32 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output"
convnext_base-script,,2024-07-12,,,,18,25.134303772007115,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.00341796875,2.0234375,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/convnext_base-script-cuda-torch.float16/model.onnx,False,True,False,True,15925248.0,x86_64,0.0,209138176.0,217657856.0,217657856.0,217657856.0,217657856.0,40370688.0,40370688.0,40370688.0,40370688.0,0,convnext_base,convnext_base,0,177382342.0,input.1,1.0,0.0,344.0,1.0,814.0,1.0,0.0,0.0,0.0,1158,144.0,,,,,144.0,,40.0,36.0,,,36.0,,1.0,,,1.0,1.0,,,,41.0,,72.0,,108.0,,,,,36.0,,,,,,,,,,154.0,,,344.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,88591464.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.8349483896895998,-0.16505161031040017,0,Timm,18,2.593202595016919,0.08936839919964162,0.07461800100087809,1.5652343110123184,0.8812397760048043,18.893019904993707,0.7579782730026636,0.07385353370045777,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=convnext_base -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- w,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convnext_base --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_da"
convnext_base-dynamo2,,2024-07-12,,,,19,41.60927177700796,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.00390625,1.9921875,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/convnext_base-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,15925248.0,x86_64,0.0,209138176.0,217657856.0,217657856.0,217657856.0,217657856.0,40370688.0,40370688.0,40370688.0,40370688.0,0,convnext_base,convnext_base,0,4739415.0,l_args_0_,1.0,196.0,344.0,1.0,4.0,1.0,0.0,0.0,0.0,model_head_1,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,344.0,,,,,,,,,,,,,,,,,,1.0,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,88591464.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.8234812687934527,-0.17651873120654726,0,Timm,18,18.70083456498105,0.09032010856705407,0.07437691760036008,1.5410076570115052,0.9248584510060027,35.43279587701545,0.8036273360979976,0.07186692370160017,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version_ please register them with register_custom_op. --   warnings.warn( -- 2024-07-12 16:40:0,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=convnext_base -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convnext_base --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_d"
convnext_base-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.adaptive_avg_pool2d'_ overload='default')>_ searched for ['aten::adaptive_avg_pool2d'_ 'adaptive_avg_pool2d_default'] and attributes ['__qualname__'_ '__name__']_ args=(add_35_ [1_ 1])_ kwargs={}_ --DEBUG--_ --SHAPE--,,,20,18.224479958997108,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,15925248.0,x86_64,0.0,209138176.0,217657856.0,217657856.0,217657856.0,,,,,,0,convnext_base,convnext_base,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,88591464.0,0,x86_64,,1,30,,,0,Timm,18,7.138095263013383,,0.07469473750000664,1.52055519001442,,,,0.07219854650029447,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=convnext_base -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- w,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model convnext_base --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_da"
crossvit_9_240-script,,2024-07-12,,,,21,12.84957705801935,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.015625,2.353515625,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/crossvit_9_240-script-cuda-torch.float16/model.onnx,False,True,False,True,44236800.0,x86_64,0.0,105598976.0,114118656.0,114118656.0,114118656.0,114118656.0,96995328.0,96995328.0,96995328.0,96995328.0,0,crossvit_9_240,crossvit_9_240,0,17444647.0,x,1.0,0.0,267.0,1.0,1804.0,1.0,0.0,0.0,0.0,2097,140.0,,,37.0,68.0,562.0,2.0,2.0,36.0,,2.0,24.0,2.0,,2.0,,2.0,,,,1.0,44.0,,120.0,,80.0,,,1.0,,52.0,1.0,17.0,,45.0,18.0,12.0,36.0,36.0,,146.0,314.0,2.0,267.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,256000.0,float16,8553296.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.6085124351871122,-0.3914875648128878,0,Timm,18,1.2958153910003603,0.06167051243343546,0.03752727370010689,0.4369599460042082,0.28150120298960246,6.652013096987503,0.08585818410210777,0.04406651060271542,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/timm/models/crossvit.py:286: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values_ so this value will be treated as a constant in the future. This means that the t,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=crossvit_9_240 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model crossvit_9_240 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_d"
crossvit_9_240-dynamo2,,2024-07-12,,,,22,28.1674734709959,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0302734375,3.572265625,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/crossvit_9_240-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,44236800.0,x86_64,0.0,105598976.0,114118656.0,114118656.0,114118656.0,114118656.0,96995328.0,96995328.0,96995328.0,96995328.0,0,crossvit_9_240,crossvit_9_240,0,30282790.0,l_args_0_,1.0,203.0,262.0,1.0,148.0,1.0,0.0,0.0,0.0,mean,2.0,,,2.0,3.0,42.0,,,,,,,,,2.0,16.0,2.0,,,,,2.0,,,,20.0,,,1.0,,,,,,2.0,,,,,,32.0,2.0,,262.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1.0,1.0,1.0,1.0,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,256000.0,float16,8553296.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.412283811758051,-0.587716188241949,0,Timm,18,15.205799507006304,0.09110663163398082,0.03756178936649424,0.4867490160104353,0.4228911929822061,21.980483223014744,0.10881901370012201,0.04551303469925187,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version_ please register them with register_custom_op. --   warnings.warn( -- ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=crossvit_9_240 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 --,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model crossvit_9_240 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_"
crossvit_9_240-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.upsample_bicubic2d'_ overload='vec')>_ searched for ['aten::upsample_bicubic2d_vec'_ 'upsample_bicubic2d_vec'] and attributes ['__qualname__'_ '__name__']_ args=(args_0_ [224_ 224]_ False_ None)_ kwargs={}_ --DEBUG--_,,,23,14.262327717995504,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,44236800.0,x86_64,0.0,105598976.0,114118656.0,114118656.0,114118656.0,,,,,,0,crossvit_9_240,crossvit_9_240,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,256000.0,float16,8553296.0,0,x86_64,,1,30,,,0,Timm,18,5.954962905001594,,0.037532065299456005,0.43618901300942525,,,,0.04422505109978374,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=crossvit_9_240 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model crossvit_9_240 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_d"
cspdarknet53-script,,2024-07-12,,,,24,14.06755958602298,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.017578125,5.625,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/cspdarknet53-script-cuda-torch.float16/model.onnx,False,True,False,True,12582912.0,x86_64,0.0,83576832.0,92096512.0,92096512.0,92096512.0,92096512.0,33686016.0,33686016.0,33686016.0,33686016.0,0,cspdarknet53,cspdarknet53,0,55448914.0,input.1,1.0,0.0,337.0,1.0,242.0,1.0,0.0,0.0,0.0,651,23.0,,67.0,,5.0,5.0,,67.0,,,,,,1.0,,,1.0,1.0,,,,,67.0,,,,,,,,,,,,,,5.0,,,,,,,337.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,27642184.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.9425695325990575,-0.057430467400942486,0,Timm,18,1.0498533290228806,0.036516228999244046,0.03441908490009761,0.7150160660094116,0.2880555989977438,7.6754617230035365,0.26813919460109903,0.048683104300289416,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=cspdarknet53 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- wa,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model cspdarknet53 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_dat"
cspdarknet53-dynamo2,,2024-07-12,,,,25,25.677171040006215,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.017578125,5.625,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/cspdarknet53-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,12582912.0,x86_64,0.0,83576832.0,92096512.0,92096512.0,92096512.0,92096512.0,33686016.0,33686016.0,33686016.0,33686016.0,0,cspdarknet53,cspdarknet53,0,687843.0,l_args_0_,1.0,39.0,337.0,1.0,6.0,1.0,0.0,0.0,0.0,model_head_1,,,1.0,,,1.0,,1.0,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,337.0,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,27642184.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.9423245133851473,-0.05767548661485267,0,Timm,18,12.736797419987852,0.03651269180021093,0.034406804533015624,0.7198801409977023,0.29855536701506935,19.359362996008713,0.2634868749009911,0.04914901159936562,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version_ please register them with register_custom_op. --   warnings.warn( -- /home/dupredupre/.loc,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=cspdarknet53 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- w,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model cspdarknet53 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_da"
cspdarknet53-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,26,15.632474762998754,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,12582912.0,x86_64,0.0,83576832.0,92096512.0,92096512.0,92096512.0,,,,,,0,cspdarknet53,cspdarknet53,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,27642184.0,0,x86_64,,1,30,,,0,Timm,18,7.102710131002823,,0.034422705799806864,0.7562999580113683,,,,0.04901787100243382,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=cspdarknet53 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- wa,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model cspdarknet53 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_dat"
deit_base_distilled_patch16_224-script,,2024-07-12,,,,27,16.05156552401604,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.013671875,3.486328125,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/deit_base_distilled_patch16_224-script-cuda-torch.float16/model.onnx,False,True,False,True,9633792.0,x86_64,0.0,204127232.0,213564416.0,213564416.0,213564416.0,213564416.0,28705280.0,28705280.0,28705280.0,28705280.0,0,deit_base_distilled_patch16_224,deit_base_distilled_patch16_224,0,174833620.0,input.1,1.0,0.0,155.0,1.0,959.0,1.0,0.0,0.0,0.0,1138,86.0,,,36.0,28.0,274.0,2.0,1.0,25.0,,2.0,12.0,2.0,,2.0,,2.0,,,,,25.0,,72.0,,50.0,,,,,27.0,,15.0,,13.0,12.0,12.0,36.0,36.0,,85.0,102.0,2.0,155.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,87338192.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.6112945589067026,-0.3887054410932974,0,Timm,18,1.9666819510166533,0.05022423846724754,0.03070180370026113,2.73815492700669,0.8384551639901474,9.605019036011072,0.06340303320030216,0.02668133490078617,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/__init__.py:1997: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values_ so this value will be treated as a constant in the future. This means that the trace ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=deit_base_distilled_patch16_224 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model deit_base_distilled_patch16_224 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype f"
deit_base_distilled_patch16_224-dynamo2,,2024-07-12,,,,28,25.5863316070172,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.017578125,4.3125,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/deit_base_distilled_patch16_224-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,9633792.0,x86_64,0.0,204127232.0,213564416.0,213564416.0,213564416.0,213564416.0,28705280.0,28705280.0,28705280.0,28705280.0,0,deit_base_distilled_patch16_224,deit_base_distilled_patch16_224,0,823863.0,l_args_0_,1.0,110.0,152.0,1.0,29.0,1.0,0.0,0.0,0.0,div,2.0,,,,1.0,14.0,,,1.0,,,,,,2.0,,2.0,,,,,1.0,,,,,,,,,,,,,2.0,,,,,,2.0,,,152.0,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,87338192.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.6201745576205764,-0.3798254423794236,0,Timm,18,13.137166325992439,0.04946104026651786,0.03067447876674123,1.4463738189951982,0.7506455460097641,19.282272090989863,0.06515638419950846,0.025056227500317618,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version_ please register them with register_custom_op. --   warnings.warn( -- ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=deit_base_distilled_patch16_224 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=1,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model deit_base_distilled_patch16_224 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype "
deit_base_distilled_patch16_224-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.unbind'_ overload='int')>_ searched for ['aten::unbind_int'_ 'unbind_int'] and attributes ['__qualname__'_ '__name__']_ args=(permute_)_ kwargs={}_ --DEBUG--_ --SHAPE--_ dynamic_objects={}_ dynamic_objects_rev={}_ dyn,,,29,12.245564906013897,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,9633792.0,x86_64,0.0,204127232.0,213564416.0,213564416.0,213564416.0,,,,,,0,deit_base_distilled_patch16_224,deit_base_distilled_patch16_224,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,87338192.0,0,x86_64,,1,30,,,0,Timm,18,3.1429730340023525,,0.03062638113333378,1.4404142129933462,,,,0.024891134598874487,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=deit_base_distilled_patch16_224 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model deit_base_distilled_patch16_224 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype f"
dla102-script,,2024-07-12,,,,30,21.931840178993298,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.03125,9.7265625,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/dla102-script-cuda-torch.float16/model.onnx,False,True,False,True,19267584.0,x86_64,0.0,109324288.0,109324288.0,109324288.0,109324288.0,109324288.0,38536192.0,38536192.0,38536192.0,38536192.0,0,dla102,dla102,0,66807167.0,input.1,1.0,0.0,523.0,1.0,380.0,1.0,0.0,0.0,0.0,1008,42.0,,105.0,,14.0,,,106.0,,,,,,1.0,,,,1.0,,,4.0,,,,6.0,,,,,101.0,,,,,,,,,,,,,,523.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,33268888.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.0250193922518893,0.025019392251889316,0,Timm,18,1.8221376480069011,0.07811881203282003,0.08007329723332077,2.35548726702109,0.3757848060049582,15.510011445992859,0.48593881640117614,0.09165871349978261,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=dla102 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmup=1,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dla102 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" -"
dla102-dynamo2,,2024-07-12,,,,31,35.22125694798888,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.03125,9.6953125,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/dla102-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,19267584.0,x86_64,0.0,109324288.0,109324288.0,109324288.0,109324288.0,109324288.0,38536192.0,38536192.0,38536192.0,38536192.0,0,dla102,dla102,0,894714.0,l_args_0_,1.0,67.0,527.0,1.0,12.0,1.0,0.0,0.0,0.0,model_flatten_1,,,,,,2.0,,1.0,,,,,,,,,,,,,,,,,,,,,1.0,,1.0,,,,,,,,,,,,,527.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1.0,1.0,1.0,,1.0,,,1.0,1.0,,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,33268888.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.0276877383844438,0.027687738384443827,0,Timm,18,16.645817118987907,0.07807354659986837,0.08023522653287121,0.8442932390025817,0.39412956198793836,28.883229898987338,0.49301605070068033,0.09172098750132136,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=dla102 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmup=,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dla102 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" "
dla102-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,32,18.42489796402515,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,19267584.0,x86_64,0.0,109324288.0,109324288.0,109324288.0,109324288.0,,,,,,0,dla102,dla102,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,33268888.0,0,x86_64,,1,30,,,0,Timm,18,7.859380829992006,,0.08030522086676986,0.8564605659921654,,,,0.09257115610234905,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=dla102 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmup=1,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dla102 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" -"
dm_nfnet_f0-script, NCHW>::BatchNorm(const onnxruntime::OpKernelInfo&) [with T = onnxruntime::MLFloat16,2024-07-12,,[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : ,,33,21.734601871983614,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/dm_nfnet_f0-script-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,200695296.0,209214976.0,209214976.0,209214976.0,209214976.0,58852352.0,,,,0,dm_nfnet_f0,dm_nfnet_f0,0,,,1.0,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,71489284.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,,,0,Timm,18,4.173669802985387,,0.15165607463374423,2.5443899820093066,,,,0.1574165072001051,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/symbolic_helper.py:1521: UserWarning: ONNX export mode is set to TrainingMode.EVAL_ but operator 'batch_norm' is set to train=True. Exporting with train=True. --   warnings.warn( -- [0;93m2024-07-12 16:45:06.318521128 [W:onnxruntime:_ const,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=dm_nfnet_f0 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- war,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dm_nfnet_f0 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data"
dm_nfnet_f0-dynamo2,,2024-07-12,,,,34,65.5685040219978,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/dm_nfnet_f0-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,200695296.0,209214976.0,209214976.0,209214976.0,209214976.0,58852352.0,58852352.0,58852352.0,58852352.0,0,dm_nfnet_f0,dm_nfnet_f0,0,6249090.0,l_args_0_,1.0,196.0,233.0,1.0,5.0,1.0,0.0,0.0,0.0,model_head_1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,233.0,,,,,,,,,,,,,,,,,1.0,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,1.0,1.0,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,71489284.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.065379943609432,0.065379943609432,0,Timm,18,31.35948836899479,0.142785402033284,0.15212070356647017,1.7478498260024935,0.640422305004904,56.88387049699668,1.1935501008003484,0.15913970049878118,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version_ please register them with register_custom_op. --   warnings.warn( -- 2024-07-12 16:45:3,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=dm_nfnet_f0 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- wa,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dm_nfnet_f0 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_dat"
dm_nfnet_f0-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten.pad'_ overload='default')>_ searched for ['aten::pad'_ 'pad_default'] and attributes ['__qualname__'_ '__name__']_ args=(args_0_ [0_ 1_ 0_ 1]_ 'constant'_ 0.0)_ kwargs={}_ --DEBUG--_ --SHAPE--_ dynamic_objects={}_ dyn,,,35,26.4900570329919,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,25165824.0,x86_64,0.0,199843328.0,208363008.0,208363008.0,208363008.0,,,,,,0,dm_nfnet_f0,dm_nfnet_f0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,71489284.0,0,x86_64,,1,30,,,0,Timm,18,8.572882720996859,,0.15229741410003045,2.836578721005935,,,,0.1617728843004443,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=dm_nfnet_f0 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- war,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dm_nfnet_f0 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data"
dpn107-script,,2024-07-12,,,,36,38.853506690997165,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.03515625,5.875,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/dpn107-script-cuda-torch.float16/model.onnx,False,True,False,True,9633792.0,x86_64,0.0,197807616.0,197807616.0,197807616.0,197807616.0,197807616.0,19268096.0,19268096.0,19268096.0,19268096.0,0,dpn107,dpn107,0,174513857.0,input.1,1.0,0.0,548.0,1.0,1154.0,1.0,0.0,0.0,0.0,1813,35.0,,111.0,,66.0,319.0,,111.0,,,,,,1.0,,,,1.0,,,8.0,,,,1.0,,,,,111.0,,,,,78.0,,,,,,,312.0,,548.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,86917800.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.9379567820902668,-0.06204321790973322,0,Timm,18,2.8183051669911947,0.12239375856685607,0.11480005593330134,2.9608143040095456,0.7535160270053893,30.776828000001842,1.522467646401492,0.12655161959992256,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=dpn107 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmup=1,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dpn107 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" -"
dpn107-dynamo2,,2024-07-12,,,,37,68.86239394999575,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.03515625,5.98828125,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/dpn107-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,9633792.0,x86_64,0.0,197807616.0,197807616.0,197807616.0,197807616.0,197807616.0,19268096.0,19268096.0,19268096.0,19268096.0,0,dpn107,dpn107,0,1196433.0,l_args_0_,1.0,36.0,556.0,1.0,6.0,1.0,0.0,0.0,0.0,model_flatten_1,,,,,,2.0,,1.0,,,,,,,,,,,,,,,,,,,,,1.0,,1.0,,,,,,,,,,,,,556.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,86917800.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.8821332691875261,-0.1178667308124739,0,Timm,18,31.53353327899822,0.13030950600029126,0.11495035053424847,3.716969647008227,0.6651785240101162,60.724958217004314,1.5435908146988369,0.1294710671994835,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=dpn107 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmup=,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dpn107 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" "
dpn107-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,38,25.192629539989866,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,9633792.0,x86_64,0.0,197807616.0,197807616.0,197807616.0,197807616.0,,,,,,0,dpn107,dpn107,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,64000.0,float16,86917800.0,0,x86_64,,1,30,,,0,Timm,18,12.202274475013837,,0.11495828346620934,1.3588877150032204,,,,0.12565851240069606,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=dpn107 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmup=1,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model dpn107 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """" -"
eca_botnext26ts_256-script,,2024-07-12,,,,39,14.772557421994861,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.05859375,22.3125,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/eca_botnext26ts_256-script-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,71812096.0,80331776.0,80331776.0,80331776.0,80331776.0,58852352.0,58852352.0,58852352.0,58852352.0,0,eca_botnext26ts_256,eca_botnext26ts_256,0,21545930.0,input.1,1.0,0.0,167.0,1.0,1269.0,1.0,0.0,0.0,0.0,1474,14.0,1.0,31.0,24.0,76.0,469.0,18.0,36.0,,,6.0,,11.0,7.0,12.0,,1.0,1.0,,,1.0,,,12.0,1.0,53.0,12.0,,5.0,,76.0,,23.0,32.0,24.0,3.0,3.0,,,12.0,36.0,263.0,6.0,167.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,10593301.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.8277401553238679,-0.1722598446761321,0,Timm,18,0.9248323859937955,0.047368458666217826,0.03920877533382736,0.9125153710192535,0.1908641760237515,8.493147519999184,0.2976556645007804,0.05215850730019156,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/__init__.py:1997: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values_ so this value will be treated as a constant in the future. This means that the trace ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=eca_botnext26ts_256 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model eca_botnext26ts_256 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --out"
eca_botnext26ts_256-dynamo2,,2024-07-12,,,,40,21.70198512999923,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0595703125,22.125,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/eca_botnext26ts_256-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,71812096.0,80331776.0,80331776.0,80331776.0,80331776.0,58852352.0,58852352.0,58852352.0,58852352.0,0,eca_botnext26ts_256,eca_botnext26ts_256,0,418082.0,l_args_0_,1.0,66.0,168.0,1.0,3.0,1.0,0.0,0.0,0.0,model_head_1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,168.0,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,10593301.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.8279224112850624,-0.1720775887149376,0,Timm,18,8.311079762002919,0.04736660786729772,0.03921587619988713,0.47020442702341825,0.2063523209944833,15.486777546000667,0.2987840618996415,0.051789792100316846,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=eca_botnext26ts_256 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model eca_botnext26ts_256 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --ou"
eca_botnext26ts_256-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,41,13.296649485011585,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,25165824.0,x86_64,0.0,71812096.0,80331776.0,80331776.0,80331776.0,,,,,,0,eca_botnext26ts_256,eca_botnext26ts_256,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,10593301.0,0,x86_64,,1,30,,,0,Timm,18,4.716446300008101,,0.03920562476754033,0.49277811899082735,,,,0.051763352399575524,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=eca_botnext26ts_256 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model eca_botnext26ts_256 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --out"
eca_halonext26ts-script,,2024-07-12,Unsupported: ONNX export of operator Unfold_ input size not accessible. ,,,42,9.133018230990274,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,script,,False,True,False,True,25165824.0,x86_64,0.0,72140800.0,80660480.0,80660480.0,80660480.0,,,,,,0,eca_halonext26ts,eca_halonext26ts,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,10756885.0,0,x86_64,,1,30,,,0,Timm,18,0.3680701039847918,,0.04053565779953108,0.6584205020044465,,,,0.053942789699067364,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/__init__.py:1997: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values_ so this value will be treated as a constant in the future. This means that the trace ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=eca_halonext26ts -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model eca_halonext26ts --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output"
eca_halonext26ts-dynamo2,,2024-07-12,,,,43,129.61774006899213,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0146484375,4.12109375,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/eca_halonext26ts-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,72140800.0,80660480.0,80660480.0,80660480.0,80660480.0,58852352.0,58852352.0,58852352.0,58852352.0,0,eca_halonext26ts,eca_halonext26ts,0,444826.0,l_args_0_,1.0,75.0,171.0,1.0,3.0,1.0,0.0,0.0,0.0,model_head_1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,171.0,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,10756885.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.015117857029898336,-0.9848821429701017,0,Timm,18,9.102562129002763,2.6821170037670528,0.0405478614004096,0.47259304599720053,0.24442876401008107,121.8421094820078,2.928850245999638,0.053993884997908026,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=eca_halonext26ts -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model eca_halonext26ts --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --outpu"
eca_halonext26ts-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,44,18.01827350698295,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,25165824.0,x86_64,0.0,72140800.0,80660480.0,80660480.0,80660480.0,,,,,,0,eca_halonext26ts,eca_halonext26ts,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,10756885.0,0,x86_64,,1,30,,,0,Timm,18,6.0926765469776,,0.04055668180032323,1.5923944099922664,,,,0.05761063470272347,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=eca_halonext26ts -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model eca_halonext26ts --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output"
,,2024-07-12,,,,45,9.3304048019927,,,,cuda,,,,float16,dump_bash_bench,0,0,,script,,,,,,,,,,,,,,,,,,0,ese_vovnet19b_dw,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,1,30,,,0,,18,,,,,,,,,1,,,,,10,"Traceback (most recent call last): --   ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=ese_vovnet19b_dw -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model ese_vovnet19b_dw --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output"
,,2024-07-12,,,,46,9.45471802999964,,,,cuda,,,,float16,dump_bash_bench,0,0,,dynamo2,,,,,,,,,,,,,,,,,,0,ese_vovnet19b_dw,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,1,30,,,0,,18,,,,,,,,,1,,,,,10,"Traceback (most recent call last): --   ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=ese_vovnet19b_dw -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 ,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model ese_vovnet19b_dw --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --outpu"
,,2024-07-12,,,,47,8.21911028100294,,,,cuda,,,,float16,dump_bash_bench,0,0,,custom,,,,,,,,,,,,,,,,,,0,ese_vovnet19b_dw,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,1,30,,,0,,18,,,,,,,,,1,,,,,10,"Traceback (most recent call last): --   ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=ese_vovnet19b_dw -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model ese_vovnet19b_dw --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output"
,,2024-07-12,,,,48,7.046634248981718,,,,cuda,,,,float16,dump_bash_bench,0,0,,script,,,,,,,,,,,,,,,,,,0,fbnetc_100,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,1,30,,,0,,18,,,,,,,,,1,,,,,10,"Traceback (most recent call last): --   ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=fbnetc_100 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warm,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model fbnetc_100 --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data "
,,2024-07-12,,,,49,8.97162845497951,,,,cuda,,,,float16,dump_bash_bench,0,0,,dynamo2,,,,,,,,,,,,,,,,,,0,fbnetc_100,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,1,30,,,0,,18,,,,,,,,,1,,,,,10,"Traceback (most recent call last): --   ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=fbnetc_100 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- war,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model fbnetc_100 --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data"
,,2024-07-12,,,,50,9.951176784990821,,,,cuda,,,,float16,dump_bash_bench,0,0,,custom,,,,,,,,,,,,,,,,,,0,fbnetc_100,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,1,30,,,0,,18,,,,,,,,,1,,,,,10,"Traceback (most recent call last): --   ",[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=fbnetc_100 -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warm,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model fbnetc_100 --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data "
fbnetv3_b-script,,2024-07-12,,,,51,28.36830968799768,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.0263671875,8.5390625,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/fbnetv3_b-script-cuda-torch.float16/model.onnx,False,True,False,True,50331648.0,x86_64,0.0,118120960.0,126640640.0,126640640.0,126640640.0,126640640.0,109185024.0,109185024.0,109185024.0,109185024.0,0,fbnetv3_b,fbnetv3_b,0,17419854.0,input.1,1.0,0.0,510.0,1.0,386.0,1.0,0.0,0.0,0.0,983,23.0,,87.0,,,18.0,,124.0,,,,,,1.0,,,1.0,1.0,18.0,77.0,,,,,,18.0,,,18.0,,,,,,,,,,,,,,,510.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,256000.0,float16,8598464.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.7855758996361454,-0.21442410036385462,0,Timm,18,1.1543377650086768,0.07268958530039527,0.05710318636653634,1.5516952270118054,0.23397476400714368,19.20625246601412,1.1030441898008576,0.07876474070071708,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=fbnetv3_b -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmu,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model fbnetv3_b --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """
fbnetv3_b-dynamo2,,2024-07-12,,,,52,43.02963325401652,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.02685546875,9.03125,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/fbnetv3_b-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,50331648.0,x86_64,0.0,118120960.0,126640640.0,126640640.0,126640640.0,126640640.0,109185024.0,109185024.0,109185024.0,109185024.0,0,fbnetv3_b,fbnetv3_b,0,794658.0,l_args_0_,1.0,54.0,510.0,1.0,11.0,1.0,0.0,0.0,0.0,model_classifier_1,,,1.0,,,3.0,,2.0,,,,,,,,,,,,2.0,,,,,,,,,1.0,,,,,,,,,,,,,,,510.0,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,256000.0,float16,8598464.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,0.7956159192653564,-0.20438408073464365,0,Timm,18,17.621542526991107,0.072156787100054,0.05740908849984407,1.1459653310012072,0.28590059297857806,34.856479337991914,1.053833055999712,0.07780877180048265,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=fbnetv3_b -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warm,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model fbnetv3_b --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data "
fbnetv3_b-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,53,19.215173937991494,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,50331648.0,x86_64,0.0,118120960.0,126640640.0,126640640.0,126640640.0,,,,,,0,fbnetv3_b,fbnetv3_b,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,256000.0,float16,8598464.0,0,x86_64,,1,30,,,0,Timm,18,9.349429066991434,,0.05710281239977728,0.7910469509952236,,,,0.07437427719996777,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=fbnetv3_b -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmu,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model fbnetv3_b --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """
gernet_l-script,,2024-07-12,,,,54,23.62072880100459,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.02734375,6.08984375,float16,dump_bash_bench,0,0,/usr/bin/python3,script,dump_test_models/gernet_l-script-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,112778752.0,121789952.0,121789952.0,121789952.0,121789952.0,59343872.0,59343872.0,59343872.0,59343872.0,0,gernet_l,gernet_l,0,62433883.0,input.1,1.0,0.0,285.0,1.0,190.0,1.0,0.0,0.0,0.0,532,18.0,,57.0,,,,,57.0,,,,,,1.0,,,1.0,1.0,,,2.0,,,,,,,,,53.0,,,,,,,,,,,,,,285.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,31078280.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.0026387334743072,0.002638733474307209,0,Timm,18,0.9997162430081517,0.03898351299964512,0.039086380100343376,1.3589085830026306,0.32001709600444883,16.987849716009805,1.1076824970979942,0.05071716920065228,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=script -- memory_peak=0 -- model=gernet_l -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmup,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model gernet_l --exporter script --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """""
gernet_l-dynamo2,,2024-07-12,,,,55,32.96536415099399,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,0.02734375,6.08984375,float16,dump_bash_bench,0,0,/usr/bin/python3,dynamo2,dump_test_models/gernet_l-dynamo2-cuda-torch.float16/model.onnx,False,True,False,True,25165824.0,x86_64,0.0,112778752.0,121789952.0,121789952.0,121789952.0,121789952.0,59343872.0,59343872.0,59343872.0,59343872.0,0,gernet_l,gernet_l,0,582840.0,l_args_0_,1.0,46.0,287.0,1.0,10.0,1.0,0.0,0.0,0.0,model_head_1,,,2.0,,,2.0,,2.0,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,287.0,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,0.0,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,31078280.0,0,x86_64,CUDAExecutionProvider_CPUExecutionProvider,1,30,1.0060952603402242,0.006095260340224229,0,Timm,18,10.978815598005895,0.038902139199975254,0.03913925786619075,0.9444847319973633,0.31741143501130864,26.194078551023267,1.0696015624009305,0.05250776920001954,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,/torch/onnx/_internal/exporter.py:137: ,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=dynamo2 -- memory_peak=0 -- model=gernet_l -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmu,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model gernet_l --exporter dynamo2 --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """
gernet_l-custom,,2024-07-12,Unable to interpret function <class 'torch._ops.OpOverload'>: <OpOverload(op='aten._native_batch_norm_legit_no_training'_ overload='default')>_ searched for ['aten::_native_batch_norm_legit_no_training'_ '_native_batch_norm_legit_no_training_default'] and attributes ['__qualname__'_ '__name__']_ arg,,,56,15.084272933017928,7.0,40.0,2024-07-12,cuda,Tesla V100-SXM2-32GB,,,float16,dump_bash_bench,0,0,/usr/bin/python3,custom,,False,True,False,True,25165824.0,x86_64,0.0,112778752.0,121789952.0,121789952.0,121789952.0,,,,,,0,gernet_l,gernet_l,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,128000.0,float16,31078280.0,0,x86_64,,1,30,,,0,Timm,18,5.858668163011316,,0.03915290866668026,1.0203721300058533,,,,0.05329554389754776,1,3.9.19,1.19.0+cu118,2.5.0.dev20240712+cu118,4.37.2,10,,[bash_bench_timm] start -- device=cuda -- dtype=float16 -- dump_folder=dump_bash_bench -- dump_ort=0 -- dynamic=0 -- exporter=custom -- memory_peak=0 -- model=gernet_l -- nvtx=0 -- opt_patterns= -- output_data= -- process=0 -- quiet=1 -- repeat=30 -- start=0 -- target_opset=18 -- verbose=1 -- warmup,"[/usr/bin/python3 -m experimental_experiment.torch_bench.bash_bench_timm --repeat 30 --warmup 10 --model gernet_l --exporter custom --process 0 --device cuda --dynamic 0 --target_opset 18 --verbose 1 --opt_patterns """" --dump_folder dump_bash_bench --quiet 1 --start 0 --dtype float16 --output_data """""
